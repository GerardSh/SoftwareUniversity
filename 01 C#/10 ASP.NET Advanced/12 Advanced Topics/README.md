# General
## `WebHost`
ASP.NET Core приложенията конфигурират и стартират хост.

Старият .NET Framework е бил тясно обвързан с Windows. Използвал е директно API-тата на операционната система. Това работи много добре, когато се поддържа само Windows, но когато възникне нужда от работа с други операционни системи, се появяват затруднения. Причината е, че приложенията комуникират директно с операционната система.
Поради това е взето решение да се създаде механизъм, който да позволява изпълнението на приложения върху различни операционни системи. Ядрото на .NET е било пренаписано така, че да бъде независимo от Windows API-тата. Създава се обща виртуална машина, която да осигурява тази независимост и възможност за работа в различни среди.
Този подход обаче усложнява комуникацията между приложенията и конкретната операционна система. Вече липсва предишната възможност за създаване на силно специализирани приложения, каквито са били например WinForms приложенията, които се изграждат бързо и лесно в рамките на Windows екосистемата.
На практика всяко ASP.NET Core приложение е конзолно приложение. Необходима е обаче абстракция, която да му придаде конкретна насоченост — тази абстракция е така нареченият хост. Хостът комуникира с операционната система и предоставя на приложението допълнителни услуги.
Хостът позволява едно и също приложение да стане специализирано в различен контекст, в зависимост от нуждите. Предоставени са различни типове хостове, един от които е т.нар. `WebHost`. Той е абстракцията, която предоставя всичко необходимо за работа с уеб технологии — HTTP заявки и отговори, сесии, пътища, query string параметри, headers, trailers и всички други аспекти, специфични за уеб приложенията.
Цялата уеб функционалност е част от Web хоста. Това позволява с промяна на хоста да се получи различен тип приложение, без да се променя бизнес логиката. Логиката обикновено се пише по един и същи начин, независимо от конкретния тип приложение.

Хостът отговаря за стартирането и управлението на жизнения цикъл на приложението. Стартира самото приложение и му предоставя определени услуги, от които то се нуждае. Една от тези услуги е управлението на жизнения цикъл (lifetime management).
Хостът трябва да може да стартира приложението, да го спре при необходимост, както и да осигури възстановяване в случай на възникнал проблем.

Минимално, хостът конфигурира сървър и pipeline за обработка на заявки.
Всеки тип хост има своя специфика. За `WebHost` това включва настройка на сървъра и request pipeline-а.
Оттам нататък се преценява какви допълнителни услуги да бъдат използвани освен задължителните. Примери за такива услуги са логване, инжектиране на зависимости, използване на конфигурации и други.
Всичко, което се конфигурира в `Program.cs`, реално настройва хоста.
Първоначално се използва host builder, в който се добавят отделни компоненти — всеки от тях представлява допълнителна услуга.
След като бъдат добавени всички необходими услуги, се извиква `builder.Build()`. Така се създава самият `WebHost`, който често се нарича web application, въпреки че реално представлява конкретен хост.
След това може да започне конфигурирането на pipeline-а, което е специфично за `WebHost`. Другите хостове притежават различни възможности и поведение.
Типът на приложението зависи от избрания хост.

Върху него вече може да почнем конфигурирането на pipeline-a. Това е специфично за web хоста, другите хостове имат други неща. От хоста зависи какъв е типа на даденото приложение.
## `WebApplication`
Преди .NET 6 първо се конфигурира `WebHost`, а след това се изгражда самото приложение.

От .NET 6 нататък тези действия се извършват едновременно в `Program.cs`.

`WebHost` по дизайн е bare minimum — нещо, което просто стартира приложението и обработва заявки, но нищо повече. Върху него започва надграждането с необходимите функционалности. Затова имаме нужда от `builder`, върху който добавяме различни услуги.
Например, `builder.Services.AddControllersWithViews()` добавя MVC поддръжка.
`builder.AddServiceDefaults()` е много добър патърн и е желателно да се спазва. Идеята е, че при няколко проекта част от настройките на `WebHost` могат да бъдат изнесени на едно централизирано място, за да не се налага да се повтарят за всяко приложение поотделно. Ако настройваме едни и същи услуги в няколко различни приложения, има риск при промяна да пропуснем някъде да я направим. Затова имаме extension метода `builder.AddServiceDefaults()`. Създаден е отделен проект `ServiceDefaults`, който съдържа един клас — `Extensions.cs`. Всичко, което пишем в `Program.cs`, може да бъде изнесено там.
Логването и мониторингът са две задължителни услуги за всяко приложение. Тяхната конфигурация е относително еднаква и затова могат да се управляват централизирано в проекта `ServiceDefaults`.

В първата част, преди да извикаме `builder.Build()`, добавяме услугите, които приложението ще ползва.
След това изграждаме pipeline-а — определяме какво трябва да се случи при заявка към приложението.

`WebApplication` представлява абстракция върху `WebHost`. Връща се от метода `Build()` на класа `WebApplicationBuilder`.

Определя начина, по който приложението комуникира със средата, в която работи.
## `CreateBuilder()`
Методът `CreateBuilder()` инициализира нова инстанция на класа `WebApplicationBuilder` и извършва няколко основни задачи:

- Конфигурира Kestrel сървъра
    
- Зарежда конфигурацията на хоста и самото приложение
    
- Настройва логването, интеграцията с IIS, content root и др.

Първоначално Kestrel не е бил проектиран като пълноценен web server, а по-скоро като _application server_, който може да стартира и обслужва уеб приложения. Това е така, защото са му липсвали редица важни механизми за сигурност, които са критично необходими за излагане на приложения в интернет.
Важно е да разбираме, че уеб пространството враждебна среда и може да се очакват атаки от различни източници. Ето защо уеб сървърът трябва да бъде добре защитен и правилно конфигуриран. Приложението ни работи върху web сървър, който в повечето случаи е Kestrel.

Макар `WebApplicationBuilder` да предоставя базова конфигурация, по подразбиране тя е минимална — например, използват се случайни портове, няма предварително зададени ограничения и политики за сигурност.

Затова е силно препоръчително да се научим как да конфигурираме Kestrel сървъра правилно. Това може да стане по няколко начина:

- Чрез подаване на аргументи при стартиране на приложението
    
- С помощта на делегат чрез метода `ConfigureKestrel`
    
- Чрез конфигурационни секции от `appsettings.json`
    
- С използване на environment променливи
    
- Или комбинация от горните подходи

Добрата конфигурация на сървъра е ключова за стабилността и сигурността на едно ASP.NET Core приложение, особено когато се разполага в production среда.

Настройва логването, интеграцията с IIS, установява кореновата директория за съдържанието и други.

Това създава стандартна конфигурация за логване, която може да бъде променяна чрез `builder.Host`. По подразбиране имаме дефинирана секция `"Logging"` в `appsettings.json` и `appsettings.Development.json`, където фреймуъркът чете настройките за конфигуриране на логването. В тази секция можем да зададем желаното ниво на логване — например, в `appsettings.Development.json` е често полезно да се сложи `Debug`, за да получаваме по-подробна информация по време на разработка.

```csharp
var builder = WebApplication.CreateBuilder(args);

builder.Host.ConfigureServices((context, services) =>
{
    services.Configure<KestrelServerOptions>(
        context.Configuration.GetSection("Kestrel"));
});

builder.Host.ConfigureLogging(logging =>
{
    logging.SetMinimumLevel(LogLevel.Warning);
});
```

По този начин настройваме логването глобално за приложението, но също така може да го презапишем с настройки от `appsettings.json`, което дава гъвкавост и лесно управление на нивата на логване според средата (development, production и т.н.).

- `WebApplication.CreateBuilder(args)` създава инстанция на `WebApplicationBuilder`, върху която след това може да се конфигурира:
    
    - Хоста: `builder.Host`
        
    - Услугите: `builder.Services`
        
    - Лога: `builder.Logging`
        
    - Конфигурации: `builder.Configuration`

- В конфигурационния файл `appsettings.json` може да има секция "Kestrel".

```json
"Kestrel": {
  "Endpoints": {
    "Http": {
      "Url": "http://localhost:5000"
    },
    "Https": {
      "Url": "https://localhost:5001"
    }
  },
  "Limits": {
    "MaxRequestBodySize": 10485760
  }
}
```

Тези настройки ще бъдат автоматично свързани към `KestrelServerOptions`, когато се ползва:

```csharp
services.Configure<KestrelServerOptions>(
    context.Configuration.GetSection("Kestrel"));
```
## Logging
**ASP.NET Core** предоставя вградена logging API, която е гъвкава и разширяема. Едно от най-важните неща в едно приложение е да имаме логове. Каквото и да се случи, за да разберем проблема, трябва да разгледаме логовете. Преди ASP.NET Core логовете обикновено са се записвали във файлове на диска или в базата данни, което е увеличавало размера на базата. От Майкрософт са решили да ни предоставят инструмент, който да е достатъчно гъвкав и да можем да управляваме логовете си според нуждите.

Фреймуъркът разполага с logging factory — фабрика, която създава логъри. Когато създадем логър, той следва определен интерфейс и позволява да записваме необходимата информация. Абстракцията е налична чрез фабрика и един общ интерфейс, което означава, че чрез смяна на провайдърите можем да променяме начина, по който логваме проблемите в приложението.

Появили се се много компании, които разработват логинг провайдъри за различни нужди. Това обаче не винаги е добре, защото при инсталиране на логинг провайдър често се добавят и много ненужни зависимости.

От Майкрософт предоставят два основни провайдъра: първият е за дебъгване — той знае как да логва в дебъг конзолата на Visual Studio. Докато разработваме, в тази конзола можем да виждаме какво се случва и да записваме нужните грешки.

Вторият провайдър логва в стандартния изход (Standard Out). Това е абстракция, която се държи различно в зависимост от средата, в която е пуснато приложението. Например:

- Ако приложението се стартира в Windows, грешките се показват в конзолата.
    
- При стартиране като Linux service, стандартният изход е насочен към системния журнал (`systemd journal`), и всички грешки се записват там.
    
- Ако приложението е в Docker контейнер, стандартният изход е насочен към Docker логовете.

Стандартният изход е универсален и всяка платформа знае къде да го насочи, така че грешките се виждат там, където е най-удобно.

Има и други варианти за логване, като например ELK stack (Elasticsearch, Logstash, Kibana), който е много подходящ за анализ на грешки и логове. Обаче ако инсталираме провайдър, който директно записва в Elasticsearch, трябва да сме наясно, че това става през HTTP заявки. HTTP е сравнително бавен и ако логваме много често, особено при дебъг, това може да забави приложението, защото всеки запис трябва да бъде изпратен по мрежата.

Ако логваме само при грешки, това не е толкова голям проблем, но при интензивно логване при дебъг, може да има сериозно влияние върху производителността.

Затова е добре да пазим стандартния изход (Standard Out) като основен канал за логване. Трудно е да се каже кое е по-бързо — дали HTTP заявките или писането на диск — тъй като това зависи от множество фактори като латентност на мрежата, натоварване на диска и други.

Обикновено системният журнал работи много добре. Ако искаме да изпращаме логовете към Elasticsearch, можем да инсталираме на машината Logstash. Той може независимо от нашето приложение да чете системния журнал, да трансформира данните, да добавя допълнителна информация и след това да ги изпраща към Elasticsearch по HTTP. По този начин нашето приложение не се забавя, защото логването и трансформацията стават в отделен процес.

Това е по-чистият и ефективен подход.

Поддържа работа с различни **logging providers** – например Console, Debug, Azure, Seq, Serilog и др.  
Logging инфраструктурата се състои от **три основни компонента**:

- **`ILoggerProvider`** – определя **къде** се извеждат лог съобщенията (конзола, файл, база и т.н.)
      ↓
- **`ILoggerFactory`** – фабрика за създаване на инстанции на `ILogger`
      ↓
- **`ILogger<T>`** – интерфейс, чрез който приложението създава лог съобщения

Приложението може да има **няколко logging провайдъра едновременно**, които работят паралелно.
### `ILogger`, `ILoggerFactory` and `ILoggerProvider`
- **`ILoggerFactory`**  
    Това е фабрика, която създава инстанции на логъри. Тя управлява и регистрира различни провайдъри за логване.
    
- **`ILogger`**  
    Интерфейсът, който използваме в нашите класове, за да пишем логове. Той "опакова" всички регистрирани логъри (логинг провайдъри) и насочва съобщенията към тях.
    
- **`ConsoleLogger`, `FileLogger` и други**  
    Това са конкретни имплементации на логъри, които изпращат логовете към определени дестинации — конзолата, файлове и т.н.
    
- **`ILoggerProvider`**  
    Провайдърите се използват за създаване на логъри, които логват към конкретна дестинация. Те се регистрират в `ILoggerFactory`.

**Как работи**

1. В `ILoggerFactory` се регистрират един или повече логинг провайдъри, например:
    
    - `AddConsole()` — добавя логване към конзолата.
        
    - `AddFile()` — добавя логване във файлове (ако е наличен такъв провайдър).
        
2. Когато инжектираме `ILogger<T>` в клас, той създава логър, който обединява всички регистрирани провайдъри и подава логовете към тях.
    
3. Чрез `CreateLogger()` фабриката създава конкретен логър, който ползваме за писане на съобщения.

**Примерно използване**

```csharp
public class MyService
{
    private readonly ILogger<MyService> _logger;

    public MyService(ILogger<MyService> logger)
    {
        _logger = logger;
    }

    public void DoWork()
    {
        _logger.LogInformation("Working...");
    }
}
```

В този пример, логърът ще изпрати логовете към всички провайдъри, регистрирани във фабриката, като например конзола, файл и други.
### Logging Configuration
Конфигурацията за логване обикновено се задава чрез настройките на приложението:

`appsettings.json`

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Warning"
    }
  },
  ...
}
```

Свойството `Logging` може да съдържа `LogLevel`.

`LogLevel` указва минималното ниво за логване.

Други свойства под Logging могат да указват доставчици на логове.

Примерни логове:

```
info: TodoApi.Controllers.TodoController[1002]
 Getting item 0

warn: TodoApi.Controllers.TodoController[4000]
 GetById(0) NOT FOUND
```
### Logging Levels
Нивата на логване не зависят от конкретна технология.

Важно е да се познават нивата и тяхното предназначение.

Когато сe избере едно ниво, се логва всичко над това ниво. Примерно  е избрано Information, ще се логват всички нива до Critical.

Нивата на логване и тяхното описание:

**Trace** – за информация, която е ценна само при отстраняване на грешки.

**Debug** – за информация, полезна по време на разработка и отстраняване на грешки.

**Information** – за проследяване на общия поток на приложението. Тук могат да се видят и SQL заявките.

**Warning** – за необичайни и неочаквани събития в потока на приложението.

**Error** – за грешки и изключения, които не могат да бъдат обработени.

**Critical** – за проблеми, изискващи незабавно внимание.
### How to Log Messages
```csharp
public class HomeController : Controller
{
    private readonly ILogger<HomeController> logger;

    public HomeController(ILogger<HomeController> logger) // Inject ILogger through the constructor
        => this.logger = logger;

    public IActionResult Index()
    {
        var message = $"Home page visited at {DateTime.Now}";
        this.logger.LogInformation(message); // Logs an informational message

        var error = "Some error";
        this.logger.LogError(error); // Logs an error message

        return View();
    }
}
```

Съобщенията се показват в конзолата.

Примерни съобщения:

```text
info: YourApp.Controllers.HomeController[0]
      Home page visited at 6/10/2025 10:42:15 AM

fail: YourApp.Controllers.HomeController[0]
      Some error
```

Разбира се къде е грешката, тъй като `ILogger<T>` е generic интерфейс, а като параметър е подаден `HomeController`.
### `OpenTelemetry` with Application Insights and .NET Aspire
Всяко приложение, дори и най-добре написаното, неминуемо се сблъсква с проблеми и бъгове. Това, което отличава добрите приложения от останалите, не е липсата на грешки, а способността ни **бързо да реагираме на възникнали проблеми** – дори **преди клиентите да ги забележат**.

За да постигнем това, е важно **да имаме пълна видимост (visibility)** върху състоянието на приложението:

- **Да можем да проследим всеки проблем,** още в момента на възникването му
    
- **Да разполагаме с логове, метрики и трасета (traces)**
    
- **Да можем бързо да разберем откъде идва проблемът** и да реагираме своевременно

**Логърът е необходим, но не е достатъчен**

Логърът (чрез `ILogger<T>`) е **основен инструмент**, който трябва да бъде **винаги достъпен и лесен за анализ**, без нужда да се рови по сървъри. Но в съвременните приложения **само логовете не са достатъчни**, особено когато:

- **Грешка не е записана в лога**, но приложението не работи коректно
    
- **Проблемът е разпределен** в няколко микросервиса
    
- **Изпълнението се забавя**, но няма видима причина

**Нужно е да проследим целия път: от заявка до отговор**

За тази цел ни е необходим **механизъм за tracing и събиране на метрики**, с който да:

- **Проследим всяка заявка по целия ѝ път през системата**
    
- **Видим колко време отнема всяка стъпка**
    
- **Сравним метрики с предходни стойности**
    
- **Открием тесни места (bottlenecks)** и ги отстраним превантивно

Например:

> Ако приложението ни е обработвало по 200 заявки в секунда, а внезапно падне до 100, без да има спад в броя на потребителите – това е ясен сигнал за проблем. Може да се окаже, че даден компонент е претоварен. Ако имаме нужната видимост, можем **да пуснем hotfix** преди потребителите да разберат.

**`OpenTelemetry` и .NET**

Microsoft предоставя **вградена поддръжка за `OpenTelemetry`** – отворен стандарт за логване, tracing и метрики. Целта е да се даде възможност за:

- Централизирано наблюдение на приложението
    
-  Интеграция с популярни инструменти като **Prometheus** и **Grafana**
    
- **Проследяване в реално време** как се държат компонентите на системата

**Проблем: повечето решения изискват сложна настройка**

Първоначално интеграцията на `OpenTelemetry` изискваше инсталиране на допълнителни инструменти и сървиси, което възпря много разработчици. Microsoft осъзнават това и започват да предоставят **по-достъпни инструменти**.

**Application Insights и .NET Aspire**

- **Application Insights** е решение на Microsoft за централизирано наблюдение, но работи основно в **Azure**.
    
- За да дадат възможност на разработчиците да наблюдават системата **локално**, Microsoft създават **.NET Aspire** – оркестратор, който:
    
    - Може да се използва **в development среда**
        
    - Позволява визуализация на логове, трасета и метрики
        
    - Осигурява **графичен интерфейс през браузъра** за бърз анализ на проблеми
        
    - В production се публикува основно в Azure, но локалното му използване спестява много време в процеса на разработка

**Как да го активираме?**

За да се възползваме от `OpenTelemetry` и .NET Aspire в нашето приложение, е нужно да добавим следната конфигурация в `Program.cs`:

```csharp
builder.ConfigureOpenTelemetry();
```

Това осигурява начална интеграция с `OpenTelemetry`, като можем да разширим конфигурацията според нуждите ни (например да добавим Prometheus exporter, графичен интерфейс и др.).

**Заключение**

В ерата на облачните и разпределени приложения **видимостта е ключова**. За да поддържаме стабилни и добре работещи системи:

- Трябва да събираме логове, трасета и метрики
    
- Трябва да можем да ги анализираме в реално време
    
- Трябва да можем да реагираме **преди потребителите** да усетят проблемите

Съчетанието от **`ILogger`, `OpenTelemetry`, .NET Aspire и подходящи визуализации** ни дава именно тази възможност.
## Cache
**An Efficient Way to Store Data**

![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250610205651.png)

Кешът създава копие на част от данните и го съхранява по-близо до мястото, където тези данни ще бъдат използвани. Това позволява те да бъдат извлечени значително по-бързо, отколкото ако се взимат всеки път от оригиналния източник.

Кеширането е мощен инструмент за подобряване на производителността. Но има една уловка: инвалидацията на кеша е толкова трудна, че редом с нея поставяме и измислянето на добри имена за променливи. Ако някога чуем „това ще е лесна задача“ – трябва да проверим дали не включва кеш.

Кешът работи най-добре с данни, които **не се променят често**. Например: трябва да показваме **валутните курсове от БНБ** за няколко валути. БНБ публикува тези курсове веднъж дневно на сайта си.
Имаме сървис, който ги тегли оттам. Но въпросът е – **всеки път ли**, когато някой от нашето приложение иска да види курса, трябва да правим заявка към БНБ?
**Няма смисъл.** Това би било излишно натоварване както за нашето приложение, така и за сървъра на БНБ. Освен това това е **web call**, който е **далечен, бавен и уязвим** на проблеми.
Затова го правим по-умно: **веднъж на ден теглим курса и го пазим в кеша**. До следващия ден просто го показваме оттам. Ако искаме да събираме и история – тогава вече си ги записваме локално.
Разбира се, **не всичко става за кеширане**. Например **стоковата борса**, където данните се сменят на всяка секунда – там кешът ще покаже остаряла информация.

```
+-----------+
|   Client  |
+-----+-----+
      |
      v
+-----+-----+
|   Cache   |
+-----+-----+
      |
   Cache hit?
   /       \
  /         \
Yes         No
 |           |
 v           v
+-------+  +--------+
| Return|  |  Query |
| Cache |  |Database|
| Data  |  |        |
+-------+  +--------+ 
                |
                v
           +---------+
           | Store in|
           |  Cache  |
           +---------+
                |
                v
          +-----------+
          |  Return   |
          |  Data to  |
          |  Client   |
          +-----------+
```

Когато имаме заявка (request), процесът с кеширане обикновено следва тези стъпки:

1. Първият път заявката **не намира резултат в кеша**.
    
2. Извиква се **базата данни** и се взима необходимата информация.
    
3. Данните **се записват в кеша**.
    
4. Резултатът се връща към клиента.
    

При следваща заявка със същите параметри:

- **Кешът се проверява първо**.
    
- Ако има резултат – **той се връща директно**.
    
- Ако няма – **заявката отново стига до базата**, данните се взимат и **се добавят в кеша**.

Това поведение се нарича **cache-aside pattern** (или lazy caching) – кешът сам не знае кога да се обнови, ние го управляваме.

Основно правило и концепции при кеширането:

> **Кешът е помощен механизъм**, а не основен източник на истина (source of truth). Загубата му **не трябва** да води до срив или невъзможност да се обслужи заявка.

Ако кешът бъде изчистен, повреден или недостъпен:

- Системата **трябва да може да продължи да работи**, макар и с по-ниска производителност.
    
- Данните се вземат от оригиналния източник (напр. база данни или външен API) и кешът се попълва отново.
### Cache Types
ASP.NET Core поддържа няколко вида кеширане:
### In-Memory Cache
Най-опростеният кеш е базиран на `IMemoryCache`. Това е кеш в паметта, който се съхранява в оперативната памет на сървъра на приложението. Информацията от паметта бива извлечена в пъти по бързо от колкото от диска или базата. Проблема е когато приложението ни работи на няколко сървъра, тогава кеша между сървърите ще се различава. Този кеш се ползва предимно за нещо малко, което работи на една инстанция.

Може да съхранява всякакъв тип данни – примитивни или комплексни обекти.

In-memory Cache се конфигурира като обикновена услуга.

Добавя се `IMemoryCache` като зависимост в DI контейнера.

```csharp
// Add the IMemoryCache as a dependency to the DI
builder.Services.AddMemoryCache();

public class HomeController : Controller
{
    private readonly IMemoryCache cache;

    public HomeController(IMemoryCache memoryCache)
    {
        // Inject the IMemoryCache through DI
        this.cache = memoryCache;
    }

    public IActionResult Index()
    {
        string cachedValue;

        // Try to get the value from cache
        if (!cache.TryGetValue("myKey", out cachedValue))
        {
            // If not found, generate or retrieve the value
            cachedValue = "Some expensive data";

			// Option 1
            // Store it in cache for future use
            cache.Set("myKey", cachedValue, TimeSpan.FromMinutes(5));

			// Option 2
			// Define cache options with sliding expiration
            var cacheEntryOptions = new MemoryCacheEntryOptions()
                .SetSlidingExpiration(TimeSpan.FromMinutes(2));

            // Store it in cache
            cache.Set("myKey", cachedValue, cacheEntryOptions);
        }

        return View(model: cachedValue);
    }
}
```

Този пример:

Проверява дали стойност с ключ `"myKey"` съществува в кеша.

Ако не съществува, се създава или извлича стойността.

Разликата между двете опции е:

- `Set(..., TimeSpan)` създава абсолютна продължителност – изтича след 5 минути независимо от достъпа.
    
- `SetSlidingExpiration(...)` удължава живота при всяко ползване – изтича 2 минути след последния достъп.

`"myKey"` е уникалният ключ, който се използва за достъп до съответната стойност в кеша.

Връща се на клиента.

Така при следващо извикване със същия ключ, стойността ще бъде взета директно от кеша, без повторна обработка.

**Втори пример:**

```csharp
public IActionResult GetCachedData()
{
    DateTime cacheEntry;

    // Look for cache key
    if (!this.cache.TryGetValue(CacheKeys.Entry, out cacheEntry))
    {
        // Key not in cache, so get data
        cacheEntry = DateTime.Now;

        // Set cache options
        var cacheEntryOptions = new MemoryCacheEntryOptions()
            // Keep in cache for this time
            // Reset time if accessed
            .SetSlidingExpiration(TimeSpan.FromSeconds(3));

        // Save data in cache
        this.cache.Set(CacheKeys.Entry, cacheEntry, cacheEntryOptions);
    }

    return View("Cache", cacheEntry);
}
```

Кешираната стойност на `DateTime` остава в кеша.

Стойността ѝ не се променя от момента на кеширането.

```html
<h3>>Current Time: @DateTime.Now.TimeOfDay.ToString()</h3>
<h3>Cached Time: @(Model == null
			 ? "No cached entry found"
             : Model.Value.TimeOfDay.ToString())
</h3>
```

Ако постоянно има заявки, кешът няма да се инвалидира, защото времето на изчакване се "рестартира" при всяка заявка (при sliding expiration). Това може да доведе до задържане на остарели данни в кеша, което не винаги е желателно. Затова е важно да се прецени дали sliding expiration е подходящ за конкретния сценарий или е по-добре да се използва абсолютен срок на валидност (absolute expiration).

Ако има много заявки с различни ключове за кеширане, това може да доведе до голямо използване на памет, защото всяка стойност се съхранява отделно в кеша. Затова е важно да се обмислят стратегии за управление на кеша, като лимитиране на размера, задаване на изтичане на данните (expiration) или използване на подходящи политики за изхвърляне (eviction).
### Distributed Cache
![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250611123640.png)

`IDistributedCache` е вид кеш, който се споделя между множество сървъри на приложението. Най-добрият вариант е да използваме Redis. Redis съхранява цялата информация в RAM паметта на специално предназначен сървър, който се използва само за тази цел. След това нашите application сървъри се обръщат към него и четат директно от неговата памет. Това е по-бързо, отколкото ако се обръщат към базата данни. Все пак информацията в кеша може да е остаряла, затова трябва да се внимава с валидността на данните.

Може да се съхраняват кеширани данни в база данни на SQL Server. При този метод отново се извършва връзка с базата, за да се изтеглят данните.
Това може да бъде малко по-бързо в сравнение със стандартно заявяване към основната таблица, но не е най-ефективното решение.

Конфигурира се Distributed Cache.

```csharp
builder.Services.AddDistributedSqlServerCache(
    options =>
    {
        options.ConnectionString = Configuration.GetConnectionString("DefaultConnection");
        options.SchemaName = "dbo";
        options.TableName = "TestCache";
    });
// Alternatively, we can use Redis for distributed caching:
// builder.Services.AddDistributedRedisCache(...) 
// or preferably:
// builder.Services.AddStackExchangeRedisCache(...)

builder.Services.AddSession();
```

Таблицата за кеш се създава чрез командата `sql-cache`.

```bash
dotnet sql-cache create "Data Source=(localdb)\\mssqllocaldb;Initial Catalog=DistCache;Integrated Security=True;" dbo TestCache
```

По-добър вариант е информацията да се съхранява в Redis вместо в базата данни. Когато работим с Redis, независимо дали за кеширане или като key-value store, по-добрият избор е да се използва клиентът на `StackExchange` Redis, а не само Microsoft-ския wrapper.
Microsoft са създали пакета `DistributedRedisCache`, като вътрешно използват драйвера на `StackExchange.Redis`. Техният драйвер е много добър, защото е изключително бърз, стабилен и добре оптимизиран за работа с Redis, като поддържа разширени функции и по-добра производителност в сравнение с други реализации.
### Cache Tag Helpers
Фреймуъркът предоставя удобен Tag Helper, който изглежда като обикновен HTML елемент, но зад него стои специализиран код. Този код проверява дали кешираното съдържание вече съществува и при наличност го връща веднага. Ако кешът липсва, той генерира съдържанието, добавя го в кеша и го показва. По този начин управлението на кеширането става автоматично и прозрачно.

Cache Tag Helper кешира съдържанието във вътрешния кеш провайдър.

```
<cache enabled="true">
 Current Time: @DateTime.Now
</cache>

<cache expires-on="new DateTime(2025,1,29,17,02,0)">
 Current Time: @DateTime.Now
</cache>

<cache>
 Current Time: @DateTime.Now
</cache>

<cache expires-after="TimeSpan.FromSeconds(120)">
 Current Time: @DateTime.Now
</cache>

<cache expires-sliding="TimeSpan.FromSeconds(60)">
 Current Time Inside Cache Tag Helper: @DateTime.Now
</cache>
```

Кешът е активиран в първия пример.

Съдържанието изтича в определена дата и час във втория пример.

Показано е стандартното поведение на кеширането в третия пример.

Съдържанието изтича след определен интервал от време в четвъртия пример.

Плъзгащото изтичане нулира таймера при достъп в петия пример.

**Без Cache Tag Helper (ръчно кеширане)**

```csharp
@{
    // Try to get cached content from memory cache
    var cacheKey = "CurrentTimeCache";
    string cachedTime = null;

    if (!MemoryCache.TryGetValue(cacheKey, out cachedTime))
    {
        // If cache does not exist, create and set it
        cachedTime = DateTime.Now.ToLongTimeString();

        var cacheEntryOptions = new MemoryCacheEntryOptions()
            .SetSlidingExpiration(TimeSpan.FromSeconds(60)); // Cache expires if not accessed for 60 seconds

        MemoryCache.Set(cacheKey, cachedTime, cacheEntryOptions);
    }
}

<h3>Current Time: @DateTime.Now.ToLongTimeString()</h3>
<h3>Cached Time: @cachedTime</h3>
```

Тук трябва ръчно да проверим дали кешираното съдържание съществува, да го създадем ако не, и да го добавим в кеша със зададени настройки. Обикновено тази логика се слага в контролера или в някой услужлив (service) клас, а не директно в Razor изгледа.

**С Cache Tag Helper (автоматично кеширане)**

```html
<h3>Current Time: @DateTime.Now.ToLongTimeString()</h3>
<cache expires-after="00:01:00">
    <h3>Cached Time: @DateTime.Now.ToLongTimeString()</h3>
</cache>
```

Тук просто обграждаме съдържанието с `<cache>` и задаваме кога да изтече кеша. Фреймуъркът автоматично управлява кеша.

При използване на Cache Tag Helper, когато кешът е конфигуриран да ползва Redis или друг външен кеш провайдър, ключът (cache key), с който се съхранява кешираното съдържание, се генерира автоматично от фреймуърка. Tag Helper-а кешира само съдържанието между `<cache>` таговете, а не целия изглед.

**Какво се спестява с Cache Tag Helper**

Ръчното кеширане изисква:

- Проверка дали кеша съществува.
    
- Създаване и конфигуриране на кеш записа.
    
- Връзка с кеш провайдъра (например, `MemoryCache`).
    
- Писане на повече код.

Cache Tag Helper ни дава:

- Лесен и чист начин за кеширане директно в Razor.
    
- Управление на изтичане на кеша с атрибути.
    
- Без допълнителен код в контролера или модела.
### HTTP Response Cache
Има и други видове кеширане, като например HTTP-базирано кеширане на отговори. Това е много важна част от оптимизацията на нашето приложение и  се използва много често. Излиза ни евтино, защото всичко, което трябва да направим, е да манипулираме хедърите на заявката. Реално ние не кешираме нищо при нас, а връщаме един header, който казва на браузъра, че тази информация не се променя често и може да я запази локално. Следващия път, когато потребителят иска да я види, браузърът я зарежда от собствения си кеш. Евентуално, когато изтече определено време, се прави нова заявка, за да се обнови съдържанието.
Като разработчици на бекенд приложението можем да управляваме този процес чрез хедърите. Принципно можем да достъпим response обекта и да добавим хедъри директно, но няма нужда – в ASP.NET имаме механизъм, който ни позволява да управляваме кеширането на браузъра, без да се налага да знаем всички детайли за хедърите.

Основният HTTP хедър за кеширане е Cache-Control.

Той се използва за задаване на директиви за кеширане.

Тези директиви управляват поведението на кеширането по време на комуникация.

Кеширането на отговори в ASP.NET Core се контролира чрез лесен за използване middleware. Нужно е единствено да направим промяна в pipeline-а, като включим компонент, който манипулира съответните хедъри. Microsoft вече са разработили такъв middleware, така че не е нужно да го пишем сами – просто трябва да го добавим към приложението.

```csharp
// HTTP Response Cache
builder.Services.AddResponseCaching(); 
app.UseResponseCaching();
```

**`AddResponseCaching()`** регистрира необходимите услуги в DI контейнера. Освен това добавя всички основни building блокчета, нужни за тази функционалност, към нашия уеб хост.

След като сме извикали **`AddResponseCaching()`**, хостът вече знае как да работи с кеширане на HTTP отговори. Но това не е достатъчно — трябва и изрично да го включим в middleware pipeline-а чрез **`app.UseResponseCaching()`**, за да започне да се прилага върху входящите заявки. За да работи правилно `UseResponseCaching()` **трябва да се извика преди `UseAuthorization()`** и **`MapControllerRoute()`**, но след **`UseRouting()`**.

След като се активира, може да се конфигурира.

Ръчно в обработващите методи на заявките.

С атрибути върху действията на контролера.

Удобният вграден атрибут `ResponseCache` е доста полезен.

```csharp
// Prevents the response from being cached anywhere
[ResponseCache(Duration = 0, Location = ResponseLocation.None, NoStore = true)]
public IActionResult Error()
{
    ...
}
```

Свойствата на атрибута `ResponseCache` се използват за конфигуриране на кеширането. Свойството `Duration` определя времето в секунди, за което отговорът ще бъде кеширан. То отговаря на `max-age` стойността в HTTP header-а `Cache-Control`.

Обобщение на възможните стойности на `ResponseCacheLocation` и тяхното поведение, представено в ясна таблица:

|Стойност|Описание|
|---|---|
|`None`|Не се позволява кеширане никъде — нито в браузъра, нито в прокси сървъри.|
|`Client`|Разрешава кеширане **само в браузъра** на клиента.|
|`Any`|Разрешава кеширане **навсякъде** — в браузъра и в междинни проксита/CDN.|

Ако данните са валидни само за конкретен потребител, използваме `ResponseCacheLocation.Client`. Ако пък са валидни за всички, използваме `ResponseCacheLocation.Any`. Това означава, че ако имаме reverse proxy, той също ще кешира отговорите и следващият, който направи заявка, няма да стигне до нашето приложение, а ще получи кеширания отговор от reverse proxy-то.

По подразбиране повечето reverse проксита кешират статичните файлове и това понякога създава проблеми — например, когато променяме статични файлове, промяната не се отразява веднага, въпреки че сме обновили приложението и сме изчистили кеша на браузъра. Причината е, че между сървъра и браузъра стои устройство — обикновено application firewall, reverse proxy или gateway — което е кеширало съдържанието и връща остарели данни.
## Sessions
**Application state**

Когато сесията се съхранява на сървъра, възниква проблем, ако приложението работи на повече от един сървър. Причината е, че сесията се пази само на единия сървър, а ако заявката попадне на друг, изведнъж сесията липсва. Един от начините да се реши това е чрез настройване на load balancer-а да използва _sticky session_ — тоест всички заявки от едно и също IP винаги да се пренасочват към един и същи сървър. Това обаче създава риск от претоварване на някой от сървърите. В определени случаи, например когато хиляди потребители в рамките на една компания излизат през едно външно IP, _sticky session_ може буквално да срине приложението. Така се губи основната идея на балансирането на натоварването, защото всички заявки се насочват към една и съща машина.

Състоянието на сесията е сценарий в ASP.NET Core за съхранение на клиентски данни.

Всеки клиент има уникален Session ID, който се съхранява от framework-a.

Данните могат да се запазят, докато клиентът разглежда приложението.

Сесийните данни се поддържат чрез кеш и се считат за временни. В сесията се пази информация, която не е релевантна към бизнес информацията.

Приложението трябва да продължи да функционира и без сесийни данни.
### Configure Session
Активиране на Session middleware и настройване на in-memory доставчик за сесии.

Конфигуриране на Session.

```csharp
// services.AddMemoryCache();
// Default in-memory cache – provides IMemoryCache
// Provides IDistributedCache
builder.Services.AddDistributedMemoryCache();

builder.Services.AddSession(options =>
{
    // Set a short timeout for easy testing
    options.IdleTimeout = TimeSpan.FromSeconds(10);
    
    // XSS security
    options.Cookie.HttpOnly = true;
});

builder.Services.AddControllersWithViews();

// UseSession() Middleware must be called before UseMvc()
app.UseSession();
```

Session middleware се активира, като се добави `UseSession()` преди извикването на MVC middleware.

Добавянето на `AddDistributedMemoryCache()` позволява използване на in-memory доставчик, който предоставя интерфейса `IDistributedCache`.

Чрез `AddSession()` се конфигурират параметрите на сесията, като например времето на неактивност и настройките на cookie-то за сигурност.
### Set and Use Session
След като състоянието на сесията е конфигурирано, `HttpContext.Session` става достъпен.

Сесиите в ASP.NET Core съхраняват стойности под формата на масив от байтове, за да се гарантира сериализация.

Може да е необходимо да се използват конкретни подходи за съхраняване на по-сложни данни.

```csharp
public IActionResult GetShoppingCart()
{
    if (this.HttpContext.Session.Get("Cart") == null)
    {
        // If no cart exists in the session, create and store a new one
        this.HttpContext.Session.SetString("Cart", JsonConvert.SerializeObject(new Cart()));
    }

    // Retrieve the cart from the session and deserialize it
    this.ViewData["Cart"] = this.HttpContext.Session.GetString("Cart") == null
        ? null
        : JsonConvert.DeserializeObject(this.HttpContext.Session.GetString("Cart"));

    return View();
}
```

Сесията се използва за съхраняване на количката, като тя първо се сериализира до низ чрез JSON. При нужда се десериализира обратно до обект.
### Extend Session
Могат да се имплементират разширения за сесията, за да се улесни работата.

```csharp
public static class SessionExtensions
{
    public static void Set<T>(this ISession session, string key, T value)
    {
        // Serialize the object to JSON and store it as a string in the session
        session.SetString(key, JsonConvert.SerializeObject(value));
    }

    public static T Get<T>(this ISession session, string key)
    {
        // Retrieve the JSON string from the session and deserialize it to the specified type
        var value = session.GetString(key);
        return value == null
            ? default(T)
            : JsonConvert.DeserializeObject<T>(value);
    }
}
```

С тези методи могат по-удобно да се задават и извличат обекти от сесията, без да се пише ръчно сериализация и десериализация при всяко използване.
## Temp Data
**Store data until it's read**

![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250612114637.png)

ASP.NET Core предоставя свойството `TempData`, което служи за временен storage. Сесията също е удобна, но тя обикновено се използва за данни, свързани с конкретен потребител — като негови характеристики или собствени свойства, които могат да се съхраняват в сесията.

`TempData` позволява да се прехвърлят данни **между две отделни HTTP заявки**, което означава, че може да се използва за предаване на информация между два различни контролера или действия.
Това е полезно именно при пренасочвания (redirects), когато клиентът прави втора заявка към друг контролер или действие, и все пак трябва да се запази някаква временна информация.
За разлика от `ViewData` или `ViewBag`, които са валидни само за една заявка, `TempData` пази данните до първото им четене, дори и през няколко заявки.
Така `TempData` служи като временна „пощенска кутия“ между две отделни заявки.

Когато искаме да съхраним временна информация, която да достъпим на по-късен етап и тя не е пряко свързана с конкретен потребител, най-удобното решение е `TempData`. Това е данни, които не са собственост на потребителя, но може да се наложи да се видят след няколко заявки. Ако искаме да запазим състояние между заявките, използваме именно `TempData`.

Това свойство съхранява данните докато бъдат прочетени. Най-често в `TempData` се пазят съобщения към потребителя, защото при използване на `ViewData` или `ViewBag`, ако има пренасочване, информацията се губи. Докато `TempData`, независимо през колко пренасочвания премине, остава запазена докато някой не я прочете. В момента на прочитане тя се маркира за изтриване и при следващата заявка се премахва.

`TempData` се използва основно в ситуации на пренасочване (`Redirect`). Например, когато добавим дадено entity в даден `Action` и накрая извикаме `RedirectToAction("Index")`, искаме на страницата `Index` да покажем съобщение, че записът е бил успешен.
В този случай не можем да използваме `ViewData` или `ViewBag`, защото при `Redirect` се връща отговор към клиента с указание да направи **нова заявка** към целевия екшън (`Index`). А `ViewData` и `ViewBag` работят само в рамките на текущата заявка – те не "оцеляват" при нова заявка.
`TempData`, от друга страна, **запазва информацията между заявките**, което я прави идеална за такива сценарии. При първата заявка записваме съобщението в `TempData`, и след `Redirect` то ще бъде налично и достъпно в следващия екшън.
Така можем да визуализираме съобщение за успешен запис дори след пренасочване.

Другата основна причина да го ползваме е, когато информацията ни трябва в няколко request-а. Примерно, ако искаме да запазим shopping cart-а, може да ползваме `TempData` и да визуализираме какво има в него с `Peek()`, а вече когато потребителят си го купи – да го изтрием.

Методите `Keep()` и `Peek()` предотвратяват изтриването на данните при тяхното преглеждане. `Keep()` се използва, ако вече сме прочели данните, но искаме да ги запазим — той премахва маркировката за изтриване, която се задава след прочит.

`TempData` може да се използва както в Razor Page модели, така и в MVC контролери.

`TempData` се реализира чрез `TempData` доставчици.

Използва се или чрез „бисквитки“, или чрез състояние на сесията.

От версия ASP.NET Core 2.0 насам, доставчикът по подразбиране за `TempData` е базиран на бисквитки.

```csharp
builder.Services.AddControllersWithViews() 
    .AddSessionStateTempDataProvider(); // Not needed when working with cookies

builder.Services.AddSession(...); // Not needed when working with cookies

// ...

app.UseSession(); // Not needed when working with cookies
```

За да се използва `TempData` със сесии, е необходимо изрично да се добави `AddSessionStateTempDataProvider()`. Това указва, че `TempData` ще съхранява данните в състоянието на сесията вместо в бисквитки.

`TempData` е **временен storage, но индивидуален за всеки потребител** и по същество е **колекция тип речник**, т.е. `IDictionary<string, object>`.

Когато даден потребител направи заявка:

- ASP.NET Core създава (или използва) **отделен storage** за неговата `TempData`.
    
- Този storage е:
    
    - или **в бисквитка** (ако се ползва `CookieTempDataProvider`) – тази бисквитка се изпраща и връща **само от този потребител**
        
    - или **в сесия** (ако се ползва `SessionStateTempDataProvider`) – сесията е обвързана със съответния клиент (по session cookie)

**Важно: `TempData` ≠ Global Storage**

- Това **не е общо място**, споделено между потребители.
    
- Ако **Потребител A** запише нещо в `TempData`, **Потребител B** няма да има достъп до него – той ще има **собствена, празна `TempData`**.

**Потребител A:**

```csharp
TempData["Msg"] = "Hello, A!";
```

**Потребител B:**

```csharp
// TempData["Msg"] will be null
```

Защото:

- Бисквитките са различни (ако се ползва cookie provider).
    
- Или сесиите са различни (ако се ползва session provider).

**Обобщено:**

| Характеристика          | TempData                                        |
| ----------------------- | ----------------------------------------------- |
| Обвързана с потребителя | ✅ Да (в сесия или бисквитка на клиента)         |
| Видима за други?        | ❌ Не                                            |
| Scope                   | Само за следваща заявка или редирект            |
| Подходяща за            | Съобщения, които трябва да оцелеят при Redirect |

**`TempData` with Cookies Workflow:**

![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250612145923.png)
### Enable and Access `TempData`
![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250612150341.png)

След активиране на `TempData`, тя може да се достъпи в:

Контролера и действията му.

Razor Page моделите.

```csharp
public IActionResult RedirectToTempData()
{
    this.TempData["Previous"] = "/Home/RedirectToTempData";
    return this.RedirectToAction("AccessTempData");
}

public IActionResult AccessTempData()
{
    // Add a HttpHeader ("Previous") with the previous Action URL
    this.HttpContext.Response.Headers.Add("Previous",
        this.TempData["Previous"].ToString());

    return this.View();
}
```

В примера `TempData` се използва, за да се предаде URL на предишно действие при пренасочване и след това се чете и добавя като HTTP header.
### Post-redirect-Get
**Post-Redirect-Get (PRG)** е дизайн патърн в уеб разработката, който ни предпазва от двойно събмитване на форми – проблем, който може да доведе до многократно записване на една и съща информация в базата данни. Това се случва, когато клиентът събмитне формата, ние я обработим и върнем резултат, но след това той натисне бутона "Back" или "Refresh" което отново изпраща формата.

За да избегнем това, следваме следните стъпки: 
браузърът изпраща POST заявка → ние връщаме **redirect** → браузърът изпраща GET заявка → клиентът получава финалния отговор.  
Така, ако клиентът натисне "Назад", последната изпълнена заявка е GET и формата не се изпраща повторно.

**PRG не предотвратява напълно възможността за повторен POST при натискане на „назад“ няколко пъти**, но:

- **Значително намалява риска** – защото последната заявка е `GET`.
    
- Браузърите **предупреждават**, ако се върнем до предишна `POST` заявка.

Ако искаме 100% гаранция, че няма да има повторно записване, дори и при повторно изпращане на `POST`, трябва да добавим и **сървърна защита**, например:

- Генериране на уникален **anti-forgery token** или **form submission ID**, който се маркира като „използван“ след първия запис.
    
- Така дори ако някой натисне „Submit“ 2 пъти, втората заявка ще бъде игнорирана.

Post-redirect-Get има ключова роля в повечето приложения.

Повторното подаване на формуляри може да бъде критично при магазинни приложения.

То може да доведе до нежелан спам от данни.

Post-redirect-Get (PRG) е патърн, който е лесен за прилагане.

```csharp
[HttpGet]
public IActionResult Create()
{
    return View(new ProductModel());
}

[HttpPost]
public IActionResult Create(ProductModel productModel)
{
    if (!ModelState.IsValid)
    {
        return View(productModel);
    }

    // Do magic with productModel
    return RedirectToAction("Details", new { id = productModel.Id });
}
```

![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250612165900.png)

Първо се подава GET заявка за извеждане на формата.

След това при изпращане на POST заявка се проверява дали моделът е валиден.

Ако не е, същата форма се визуализира отново с валидираните данни.

Ако моделът е валиден, се извършва логиката (запис, изчисление и др.), и потребителят се **пренасочва** към нова страница с GET заявка (например `Details`).

Това предпазва от повторно изпращане на данните при презареждане на страницата.

`TempData` върши перфектна работа в този случай, защото запазва данните **през редиректа**.

Това означава, че може да се прехвърли например съобщение за успех, предупреждение или друга временна информация от POST действието към GET действието, без да се губи при пренасочването.

`TempData` е специално създаден за сценарии като Post-Redirect-Get, където е важно да се предадат данни между две отделни HTTP заявки.
## Areas
Някои приложения могат да съдържат голям брой компоненти. Например, в новинарски сайт потребителите могат да разглеждат и четат новини, но нямат право да публикуват съдържание. В по-простите случаи това може лесно да се управлява чрез роли. 

- **`Area`** в ASP.NET Core/MVC е **структурен инструмент** – тя организира проекта на логически части (напр. `Public`, `Admin`, `Store`, `Support` и т.н.). Това улеснява поддръжката на големи приложения и разделя отговорностите.
    
- **`Roles`** са **механизъм за достъп (authorization)** – те определят **кой има право да достъпва** даден контролер, екшън или цяла Area.

**Area организира**, **ролята контролира достъпа**. Двете работят заедно, но нито едното не замества другото.

Когато приложението нарасне и включва множество функционалности — както публични (като четене на новини), така и вътрешни (като качване на новини, генериране на справки, управление на съдържание и т.н.) — процесите стават по-сложни и се налага по-добра организация.

Тогава на практика се оформя нуждата от **отделно администраторско приложение**. Това може да бъде реализирано като самостоятелно приложение, но в зависимост от мащаба, често не е необходимо да се създават две напълно отделни приложения. Вместо това, можем да използваме **Areas** в ASP.NET MVC, за да разделим публичната и административната част на приложението.

Тази структура позволява да запазим функционална и визуална независимост между различните зони на сайта. Например, публичната част често получава повече внимание към дизайна и потребителското изживяване, докато администраторската част е по-изчистена, но силно функционална. За да не смесваме тези две среди, използваме **две отделни Areas** — например: _Public_ и _Administration_.

Всяка Area има:

- собствена структура с контролери, изгледи и модели;
    
- собствен дизайн;
    
- възможност за отделна защита — например с атрибута `[Authorize(Roles = "Admin")]` можем да ограничим достъпа само до администратори.

Това създава усещането, че работим с две отделни приложения, докато реално всичко се управлява в рамките на едно.

**Пример:**

Голямо приложение за електронна търговия, което включва:

- Магазин (Store)
    
- Потребители (Users)
    
- Блог (Blog)
    
- Форум (Forum)
    
- Администрация (Administration)

Всички тези модули могат да бъдат организирани в отделни Areas.

За да се използват Areas, трябва да се промени стандартният маршрут шаблон:

```csharp
routes.MapRoute(
    name: "areas",
    template: "{area:exists}/{controller=Home}/{action=Index}/{id?}");
```

Този маршрут позволява на приложението да разпознава заявки към различни Areas, но само ако съответната Area съществува.
## Performance
Производителността е важна тема в разработката на уеб приложения.

Бавно зареждане създава неудобство за клиентите и ги отблъсква.

Няма магическа функционалност, която автоматично да оптимизира приложението.

Има обаче много насоки как да се ускори едно приложение.

Принципно има софтуери които следят различните метрики и при отклонения да реагира и да прати аларми или да задейства действия. 

Когато искаме да правим оптимизации, е много важно да наблюдаваме приложението, за да разберем кога се налага да променим нещо. Например, ако видим, че нещо започва да се забавя, трябва моментално да реагираме и да опитаме да оптимизираме. Поначало заявките към базата може да не са били много ефективни, но когато има малко данни, те минават достатъчно бързо.

Когато системата започне да натрупва повече данни, графично можем ясно да видим деградацията в производителността. Тогава веднага трябва да вземем мерки – да прегледаме логовете, да установим коя заявка колко време е отнела, да идентифицираме бавните заявки и да ги оправим, като паралелно следим общия перформанс.

Всяко приложение деградира с течение на времето, защото се трупат данни и потребителската база расте, а това неизбежно води до спад в производителността. Понякога, докато разследваме причината за деградацията, можем временно да добавим допълнителни ресурси под формата на още сървъри, които да компенсират проблема, докато го открием и отстраним. Това се прави, за да не изпитват потребителите забавяния или проблеми.

Най-добрият вариант, разбира се, е да хванем проблема още преди да се е появил.
### Tips
#### Measure Everything
Измерваме всичко (Application Insights, `dotTrace`). Има много безплатни инструменти, които са с отворен код, например Grafana и Prometheus.

Други популярни инструменти с отворен код за мониторинг и диагностика са:

- **`Elastic Stack (ELK)`** – Elasticsearch, Logstash и Kibana за лог анализ и визуализация.
    
- **`Jaeger`** – за разпределено проследяване (distributed tracing).
    
- **`Zipkin`** – друг инструмент за проследяване на заявки в разпределени системи.
    
- **`OpenTelemetry`** – стандарт и набор от инструменти за събиране на телеметрия от приложения.

Събираме диагностична информация за приложението.

Локализираме кои са бавните компоненти на приложението.

Анализираме какво забавя тези компоненти.

Подреждаме и приоритизираме най-опасните забавяния.

Започваме с лесно постижимите подобрения.

След като определим най-бавния компонент, му даваме най-висок приоритет.

Първо се фокусираме върху лесните за отстраняване проблеми, след което преминаваме към по-сложните.
#### Enable Compression
HTTP протоколът не е особено ефективен.

Може да се включи Response Compression, за да се увеличи ефективността на приложението.

Ще има по-малко данни, които се прехвърлят през мрежата, и това ще ускори работата.

Има допълнително натоварване (overhead) от компресирането и декомпресирането, тъй като клиентът и сървърът трябва да извършват тези операции.

Обикновено обаче това натоварване е по-малко от забавянето, което се дължи на самата мрежа, затова е препоръчително да се използва, но да се следи внимателно.

```csharp
// ConfigureServices
services.AddResponseCompression();

// Configure
app.UseResponseCompression();
```
#### Reduce HTTP Requests
Комуникацията през HTTP е сравнително бавна. 

Целим се да намалим броя на HTTP заявките, необходими за изпълнение на дадена функционалност.

Примерно, може да се използва кеширане, което върши много добра работа в тази насока, стига да е приложимо в конкретния случай.

В някои ситуации не е необходимо да се намалява броят на заявките, а вместо това може да се работи върху тяхната скорост.

Например, чрез използване на Redis може значително да се намали времето за отговор.

Целта не винаги е да се намали броят на заявките, а да се намали латентността – т.е. всяка заявка да се изпълнява възможно най-бързо.

Използваме sprites за изображения и шрифтове вместо отделни изображения.
#### HTTP/2 over SSL (enabled by default)
![](https://github.com/GerardSh/SoftwareUniversity/blob/main/99%20Attachments/Pasted%20image%2020250612212235.png)

При HTTP/2 имаме компресия не само на самата заявка, но и на хедърите. Най-голямото предимство е, че се отваря една връзка (connection), в която се поддържа мултиплексиране на потоците (streams).

При HTTP/1.1, за да намалим броя на HTTP заявките, е било необходимо да бъдат бъндълвани (обединявани) всички файлове – например всички JavaScript файлове се събират в един. Този файл се минифицира – променливите се съкращават до еднобуквени имена, премахват се интервали и нови редове, за да стане колкото може по-малък. След това се изпраща към браузъра, който може да го обработи без форматиране. Това се е правило, за да се намали броят на заявките към сървъра.

Сега обаче, с HTTP/2, тази практика вече не е задължителна, защото HTTP/2 използва една връзка и изпраща всички файлове паралелно чрез мултиплексиране. Няма допълнителен overhead от отваряне и затваряне на множество връзки, затова дали файловете са бъндълнати или не, губи значение. Ако файловете не бяха бъндълнати при HTTP/1.1, щяха да се отворят няколко връзки, което забавя зареждането. При HTTP/2 пък се използва само една връзка, независимо от броя на файловете.

Все пак можем да минифицираме файловете, за да намалим обема на данните, но вече няма нужда да са събрани в един общ файл.

Обобщение:

- HTTP/2 използва двоичен протокол и компресиране на хедърите.
    
- Поддържа мултиплексиране на заявки и е съвместим с HTTP 1.1.
    
- Добра практика е да се минифицират файловете (например с `bundleconfig.json`).
    
- Компресирането е ефективен инструмент за оптимизация.
    
- Външните (third-party) ресурси могат ненужно да забавят приложението. Също могат да се минифицират, за да се намали трафикът на данни. Те могат и да бъдат силно кеширани – когато връщаме библиотека, може да задаваме `cache-control`, за да се пази дълго време, тъй като обикновено не се променя. Още по-добре е да се зарежда от CDN, защото по този начин няма да товарим нашия сървър, а ресурсът ще се изтегля от външен доставчик. Проблемът при това е, че не е под наш контрол – ако кодът бъде подменен с malicious код, клиентът може да го изтегли и изпълни. За предпазване от това може да се използва hash, като браузърът при всяко теглене сравнява съдържанието с очаквания hash и използва ресурса само ако те съвпадат.
#### Load CSS First
CSS съдържанието трябва да се зарежда първо, най-добре в секцията `<head>`, защото браузърът започва да стилизира HTML директно в правилния вид, тъй като вече знае CSS-а. Ако CSS е зареден по-надолу, браузърът първо визуализира HTML без стилове, след което прилага CSS и прави нова визуализация, което води до значително забавяне.

Браузърите имат склонност да изпълняват излишни действия при рендиране на страници.
#### Load JavaScript Last
JavaScript трябва винаги да се поставя най-отдолу, защото браузърите „не му вярват“ и спират рендирането, когато го срещнат. В момента, в който браузърът види JavaScript, той спира зареждането и рендирането на страницата и започва да парсва скриптовете. Цялото рендиране зависи от това дали JavaScript е валиден, тъй като той трябва да се изпълни върху HTML-а.

За да избегнем забавянето, JavaScript се слага най-отдолу. Така браузърът първо зарежда цялата страница и след това анализира JavaScript. За потребителя това изглежда по-добре, защото страницата се визуализира бързо и може да започне да я разглежда, докато JavaScript се зарежда на заден план — който не е необходим за самото рендиране, а за допълнителна функционалност.

Разбира се, това важи основно за сайтове, които не използват тежък JavaScript. При страници като тези, направени с React или Angular, където всичко се случва чрез JavaScript, този подход не е приложим.
#### Cache the Pages
В уеб приложенията има много статично и неизменно съдържание.

Процесът на бавното му зареждане не трябва да се повтаря.
#### Content Delivery Network (CDN)
CDN прехвърля част от натоварването от нашето приложение.

Има много CDN мрежи, разположени близо до клиентите.

CDN-ите са предпочитан ресурс в продукционна среда.

CDN е мрежа от сървъри, разпределени географски на различни места по света, която има за цел да доставя съдържание (като уеб страници, изображения, видео, JavaScript и CSS файлове) по-бързо и ефективно до крайните потребители.

Вместо всички потребители да теглят съдържанието директно от основния сървър на уебсайта (който може да е на едно място и да се забавя при много натоварване или при голямо разстояние), CDN съхранява копия на съдържанието на множество сървъри, разположени близо до потребителите.

Когато някой поиска дадена страница или файл, заявката се обслужва от най-близкия до него CDN сървър, което значително намалява времето за зареждане и натоварването на основния сървър.

Ползи от CDN:

- По-бързо зареждане на сайта и съдържанието.
    
- Намалява забавянията и латентността.
    
- По-малко натоварване върху основния сървър.
    
- По-добра устойчивост при голям трафик и DDoS атаки.
    
- Подобрява глобалното потребителско изживяване, особено при сайтове с международна аудитория.

CDN е като мрежа от дубликати или копия на съдържанието на нашия основен сървър, но тези копия са разпределени на много места по света.

Вместо потребителят да тегли файла директно от нашия централен сървър, той го взима от най-близкия до него CDN сървър.

Това намалява времето за зареждане, защото пътят на данните е по-кратък и натоварването на основния сървър намалява.

Така CDN работи като "разпръснати складове" с копия на съдържанието, които доставят по-бързо и по-ефективно до потребителите.

Накратко, CDN прави уеб сайтовете по-бързи и по-надеждни за потребителите, като „разпределя“ съдържанието по-близо до тях.
## SEO
**Search Engine Optimization (SEO)**

Search Engine Optimization (SEO) е много важен за уеб приложенията

Повечето потребители използват търсачки като **Google** или **Bing**, за да намират услуги и сайтове.

Има начини да подобрим позицията си в резултатите на търсачките (Search Engines).

Алгоритмите, използвани от търсачките, се променят често и SEO специалистите трябва непрекъснато да се адаптират към тях. Въпреки динамичните промени, съществуват утвърдени практики, които остават относително стабилни във времето.

Някои основни и ефективни насоки включват:

- Създаване на **уникално и стойностно съдържание** – копираното съдържание от други източници може да се отрази негативно на позиционирането в търсачките.
    
- Получаване на **външни връзки (backlinks)** от **надеждни и авторитетни сайтове** – двустранните линкове (където двата сайта си препращат трафик) обикновено се разглеждат като манипулация и могат да бъдат санкционирани. Връзките от сайтове с висок авторитет имат по-голяма стойност, докато такива от непознати или слабо оценени източници са с минимален или никакъв ефект.
    
- Осигуряване на **възможност за индексиране (crawlability)** на съдържанието от търсачките – сайтът трябва да бъде структуриран така, че да може лесно да бъде обходен от т.нар. "паяци".
    
- Оптимизиране на **времето за зареждане на страниците** – съдържанието трябва да се визуализира възможно най-бързо. Търсачките не чакат дълго, защото трябва да обходят публичната част от интернет и може да пропуснат части от сайта, особено ако зареждането е бавно или се изисква много JavaScript. Въпреки че съвременните "crawler-и" вече могат да интерпретират JavaScript, е препоръчително да се избягва прекомерно забавяне. Предоставянето на **sitemap** улеснява процеса на индексиране, като насочва търсачките към всички важни страници, вместо те да ги откриват на случаен принцип.
    
- Използване на **смислени и четими URL адреси** – адреси със съдържателни думи се класират по-добре от такива, които съдържат безсмислени символи или ID номера.
    
- Добавяне на **уникални и релевантни заглавия (title)** и **мета описания (meta description)** с подходящи ключови думи – макар и някои мета елементи вече да не се използват директно за класиране, те все още играят роля за разпознаване на темата на страницата и повишават нейната семантична стойност.
## GDPR
**General Data Protection Regulation (GDPR)** е регламент в правото на Европейския съюз, който:

- Регулира защитата на личните данни и поверителността на физическите лица в рамките на ЕС
    
- Обхваща също така и **трансфера на лични данни извън ЕС**. Понякога това може да представлява проблем, ако деплойваме приложението в дата център, който се намира извън Европейския съюз или извън България. Например, ако използваме Azure, но дата центърът е в Китай или САЩ, там няма същата регулация, която изисква защита на личните данни, затова нямаме право да изнасяме лични данни без съответните мерки. За някои категории данни има изключения, но е важно да сме внимателни и да спазваме регулациите

Целите на GDPR включват:

- Предоставяне на **повече контрол на потребителите** върху техните лични данни
    
- **Улесняване на регулаторната рамка** за международни бизнеси, работещи с лични данни

**ASP.NET Core** предлага API-интерфейси, които подпомагат спазването на някои изисквания на GDPR.  
Има и **примерно приложение**, публикувано в GitHub - [AspNetCore.Docs/aspnetcore/security/gdpr/sample at live · dotnet/AspNetCore.Docs](https://github.com/dotnet/AspNetCore.Docs/tree/live/aspnetcore/security/gdpr/sample)

Съгласно **GDPR**, има няколко индивидуални права, които трябва да бъдат осигурени на клиентите:

- **Право на информираност** – клиентите трябва да бъдат уведомени как се използват техните лични данни
    
- **Право на достъп** – при запитване клиентът трябва да получи достъп до личните си данни, които са събрани за него. В ASP.NET има вграден механизъм за това, но често е необходимо да го разширим, за да включва всички данни, които обработваме
    
- **Право на коригиране** – трябва да се предостави възможност за коригиране на неточни или непълни лични данни
    
- **Право на изтриване** („правото да бъдеш забравен“) – клиентите трябва да могат да поискат изтриване на своите лични данни. При това изтриваме данните на потребителя, но не и всички негови действия или записи, които са необходими за работата на системата. Обикновено, когато клиент поиска изтриване на личните си данни, ние изтриваме чувствителната информация, но запазваме някои данни като неговото ID или други анонимизирани записи. Причината е, че тези данни могат да са необходими за отчетност, логове, статистика или други бизнес / правни цели
    
- **Право на ограничаване на обработката** – клиентите могат да поискат временно блокиране на обработката на техните данни
    
- **Право на преносимост на данните** – клиентите трябва да могат да получат и използват повторно своите лични данни в структуриран и машинно четим формат
    
- **Право на възражение** – клиентите могат да възразят срещу обработката на техните лични данни, включително за маркетингови цели
    
- **Права, свързани с автоматизирано вземане на решения**, включително профилиране – клиентите имат право да не бъдат обект на решения, взети изцяло автоматизирано, ако те имат съществено въздействие върху тях
# Misc
# ChatGPT
## Nested Extension Methods
- **Extension methods are static methods** that take the instance as their first parameter, marked with `this`.
    
- When you call an extension method like `instance.Method()`, it’s just syntactic sugar for `StaticClass.Method(instance)`.
    
- Inside an extension method, calling another extension method on the same instance is simply calling another static method and passing the same instance explicitly.
    
- The `this` parameter always represents the instance you operate on, and it is passed along explicitly under the hood in every nested call.
    
- This means chaining extension methods or calling one extension method inside another is just a series of static method calls, each receiving the same object instance.
    
- You don’t need to worry about “where the instance comes from” inside extension methods — it’s always passed explicitly as a parameter.

**Example**

Original extension method using extension syntax:

```csharp
public static TBuilder AddServiceDefaults<TBuilder>(this TBuilder builder)
{
    builder.ConfigureOpenTelemetry();  // calling another extension method
    builder.AddDefaultHealthChecks();  // calling another extension method
    return builder;
}
```

What happens under the hood, rewritten explicitly as static method calls:

```csharp
public static TBuilder AddServiceDefaults<TBuilder>(this TBuilder builder)
{
    Extensions.ConfigureOpenTelemetry(builder);   // passing instance explicitly
    Extensions.AddDefaultHealthChecks(builder);   // passing instance explicitly
    return builder;
}
```

This shows clearly that the same instance (`builder`) is passed around explicitly as a parameter in each call, even though extension methods let you write it with a nicer, instance-style syntax.
## .NET Configuration Precedence and Overrides
in .NET, **configuration is a composite of multiple sources**, such as:

- `appsettings.json`
    
- `appsettings.{Environment}.json`
    
- `secrets.json` (User secrets)
    
- Environment variables
    
- Command-line arguments
    
- In-memory collections
    
- Azure Key Vault, etc.

✅ If the same **section/key exists in multiple sources**, here's what happens:

**The value from the source with the highest precedence overrides the others.**

📊 **Configuration source precedence (lowest to highest):**

|Precedence|Source|
|---|---|
|1|`appsettings.json`|
|2|`appsettings.{Environment}.json`|
|3|User secrets|
|4|Environment variables|
|5|Command-line arguments|
|6|In-memory configuration|

🧪 **Example:**

Suppose you have this in `appsettings.json`:

```json
"MySettings": {
  "ApiKey": "1234"
}
```

And this in `appsettings.Development.json`:

```json
"MySettings": {
  "ApiKey": "DEV-5678"
}
```

And you set an environment variable:

```
MySettings__ApiKey=ENV-9999
```

The final value resolved for `MySettings:ApiKey` will be:

```
"ENV-9999"
```

❗**What if the same section appears twice in one source?**

If a section appears **multiple times in a single source** (like `appsettings.json`), the **last one wins**.

**Example:**

```json
"MySettings": {
  "ApiKey": "First"
},
"MySettings": {
  "ApiKey": "Second"
}
```

👉 The final value will be `"Second"` (but this is bad practice — it can lead to unpredictable behavior or even parsing errors depending on the format).

💡 **Best Practice:**

- Avoid repeating the same section/key in the same file.
    
- Use **overrides intentionally** in higher-precedence sources like environment variables or `appsettings.{Env}.json`.
    
- Use `.Bind()` or `IOptions<T>` to load and validate settings in a structured way.
## Cached Data Storage
In distributed caching systems (e.g., using `IDistributedCache`), cached data is stored as a dictionary where the **key** is a `string` (defined by the developer), and the **value** is a `byte[]`.

Although objects are typically serialized to a `string` format like JSON or XML first, they are then converted to a byte array before being stored. This is because:

- Caching systems and databases are optimized to store and retrieve binary data.
    
- `byte[]` allows storing any type of data, not just text.
    
- It avoids encoding issues and ensures consistent performance.

During retrieval, the byte array is decoded back into a string and then deserialized into the original object. This process enables efficient, flexible, and format-agnostic caching.
## Session Store
In a **session-based authentication approach**, the client receives only a **session ID** stored in a cookie (typically named `.AspNetCore.Session`). This session ID acts as a reference key to server-side data.

The actual user data — whether it's just the user ID, or the full set of claims — is stored on the **server**, in a **session store** (in-memory, Redis, SQL Server, etc.).

📦 **What This Means:**

- **Client holds:** Just a small, opaque session ID.
    
- **Server holds:** All the actual user data (claims, roles, preferences, etc.).
    
- **Middleware can:** Use the session ID from the cookie to load the user data, reconstruct a `ClaimsPrincipal`, and assign it to `HttpContext.User`.

✅ **Advantages:**

- **Smaller cookies:** Only a short ID is sent with each request, instead of a full encrypted token or claims blob.
    
- **Improved security:** Sensitive information (e.g. roles, claims) never leaves the server, reducing exposure risk if cookies are intercepted.
    
- **Easy invalidation:** Simply remove or expire the session server-side, and the user will be logged out (no need to revoke tokens or cookies).

❌ **Trade-offs:**

- **Server memory/IO load:** Every request may involve reading from a session store.
    
- **Scalability concerns:** In distributed environments, you must use a distributed session store (like Redis), or else session affinity ("sticky sessions") must be enabled.
    
- **Manual work:** You lose the built-in benefits of cookie middleware, like automatic claim deserialization, expiration, sliding expiration, etc. You need to handle them yourself.

✅ **Summary:**

The idea is to store only a session ID in the cookie and keep all user-related data on the server, using that ID as a reference. This reduces cookie size, improves security by avoiding exposure of claims to the client, and gives you full control over what is stored and how it's managed — at the cost of added complexity and scalability considerations.
## Sessions, Claims, and Authentication in ASP.NET
- **Session Storage** in ASP.NET is a server-side mechanism that stores arbitrary key-value data per user session. It can hold **any information** you want, including user IDs, usernames, preferences, or even the entire set of user claims.
    
- When a user logs in, the server typically:
    
    - Creates a session in memory (or in distributed session store).
        
    - Stores data such as `UserId` or even user claims in the session.
        
    - Sends a **session ID** to the client via a cookie.
        
- **However, storing claims or user info in the session does NOT automatically create a `ClaimsPrincipal` for the request.**
    
- The `ClaimsPrincipal` (accessible via `HttpContext.User`) is the .NET object that represents the authenticated user and their claims. It is used throughout the ASP.NET pipeline for authorization checks and user info access.
    
- The `ClaimsPrincipal` is **normally created by authentication middleware**, such as `CookieAuthenticationMiddleware`:
    
    - This middleware reads the authentication cookie on each incoming request.
        
    - It decrypts and validates the cookie, extracting the user’s claims.
        
    - Then it creates a `ClaimsPrincipal` with those claims and assigns it to `HttpContext.User`.
        
- If you rely only on **sessions** for storing user data and **do not use authentication middleware**, then:
    
    - Even if the session contains user claims, **`HttpContext.User` will remain empty or anonymous** because no middleware populates it.
        
    - Your app would have to **manually read session data** and create user identity and claims, which is uncommon and inefficient.
        
- Using **cookie-based authentication with claims** is the standard and recommended approach because:
    
    - It automatically creates and populates `HttpContext.User`.
        
    - It avoids hitting the database or session store on every request to rebuild user identity.
        
    - It scales better across multiple servers, especially if cookies are shared or validated via a common data protection key ring.
        
- Storing just the `UserId` in the session and then querying the database on every request to load claims is possible but **inefficient** and defeats some benefits of cookie authentication.
    
- In multi-server environments, **session state can cause problems** if it is stored in-memory on one server but the request is routed to another. This can be mitigated by:
    
    - Using distributed session stores (e.g., Redis, SQL Server).
        
    - Or better yet, relying on cookie-based authentication that does not depend on server-side session state.
        
- **In summary:**
    
    - **Session store:** flexible for arbitrary user data but doesn’t automatically create user principal.
        
    - **Authentication middleware:** responsible for creating the `ClaimsPrincipal` from secure cookies.
        
    - Combining both is possible: session for custom data, cookies for identity.
        
    - For correct `HttpContext.User` population, **use cookie authentication middleware**.

**You said:**

We can create a custom middleware that builds the `ClaimsPrincipal` from the session, effectively mimicking cookie-based authentication. By storing all the user's claims in the session store during login, the middleware can retrieve them on each incoming request—using the session ID—and reconstruct the `ClaimsPrincipal`. This allows us to populate `HttpContext.User` just like cookie authentication does, but with all the data kept securely on the server side.

**ChatGPT said:**

Yes, exactly — you're spot on! Here's a breakdown of your idea and how it works in practice:

✅ **Yes, You Can Create Middleware That Builds `ClaimsPrincipal` from Session**

If you store **all user claims** in the session (e.g. on login), then you can write **custom middleware** that:

1. Runs early in the pipeline.
    
2. Reads the session (using the session ID cookie).
    
3. Retrieves the claims from session storage.
    
4. Reconstructs a `ClaimsIdentity` and `ClaimsPrincipal`.
    
5. Assigns it to `HttpContext.User`.

This effectively mimics what **Cookie Authentication Middleware** does — except your identity data is stored server-side (in session) instead of client-side (in an encrypted cookie).

🔁 **How It Would Work**

1. **Login Action:**
    
    - User logs in.
        
    - Server authenticates them and **stores their claims (or a serialized `ClaimsPrincipal`)** in the session store (e.g. `HttpContext.Session.Set("Claims", ...)`).
        
2. **Custom Middleware on Each Request:**
    
    - ASP.NET loads the session using the session ID cookie.
        
    - Your middleware checks if claims exist in the session.
        
    - If yes, it builds a `ClaimsIdentity` and sets `HttpContext.User`.

⚠️ **Pros and Cons**

✅ **Pros:**

- Centralized server-side identity management (no data in the browser).
    
- Easy to invalidate or modify identity by clearing the session.
    
- Could work well in tightly controlled systems or intranet apps.

❌ **Cons:**

- **Not scalable** unless session is stored in a **distributed store** (e.g., Redis, SQL Server).
    
- Extra **I/O** (session retrieval + deserialization) on every request.
    
- Bypasses the standard ASP.NET authentication pipeline, which can lead to unexpected behavior or compatibility issues.
    
- Manual maintenance — you’ll need to implement role/claim expiry, validation, etc., yourself.

🧠 **Conclusion**

Yes, you _can_ build a middleware that creates `HttpContext.User` from session-stored claims. It can work like cookie authentication, but cookie authentication is more **standard**, **stateless**, **performant**, and **scalable**.

Unless you have a **very specific reason** to use session-based identity (e.g., ultra-sensitive data, or strict server-only control), using **cookie authentication middleware** is almost always the better option.

**You said:**

Can we store not only the session ID in a cookie but also any information we want? Does that mean we don’t need to create a session store on the server, and instead we can create a session that holds any data directly inside the cookie? And is this different from an authentication cookie in ASP.NET, which only stores claims and roles, while a session cookie can store anything we want? Basically, there are two approaches: a Server Session and a Cookie-based Session.

**ChatGPT said:**

Great questions! Let me clarify the differences and possibilities regarding cookies, sessions, and authentication in ASP.NET:

1. **Can we store anything we want in a cookie (including session data)?**

Technically, **yes**, you can store any data you want in a cookie, as long as:

- The total size stays under browser limits (usually about 4KB per cookie).
    
- You serialize the data properly (e.g., JSON).
    
- You consider security implications (don't store sensitive data as plain text).

2. **Is it true that if we store data directly in the cookie, we don't need a session store on the server?**

**Yes, if you store all session data inside the cookie**, you don't need a separate server-side session store.

- This is called a **"cookie-based session"** or **stateless session**, since all data is stored client-side.
    
- The server just reads the cookie on each request and extracts session data.
    
- This approach can reduce server memory/storage usage.
    
- But it has limits in size and security risks if not encrypted/signed.

3. **How is this different from traditional session management?**

- **Traditional server session:**  
    Server generates a `SessionID` and stores all session data in memory or a database keyed by that ID.  
    Cookie stores only the `SessionID`.  
    Server reads session data by looking up the ID.
    
- **Cookie-based session:**  
    All session data stored **inside** the cookie itself.  
    No server session store required.

4. **What about authentication cookies in ASP.NET?**

- The **authentication cookie** (e.g., using ASP.NET Identity or cookie authentication middleware) typically stores a **security token** (like a serialized and encrypted ClaimsPrincipal).
    
- This token holds **claims, roles, user identity**, and some metadata.
    
- It **does not store arbitrary session data**; it is specifically for authentication and authorization.
    
- This cookie is **signed and encrypted** to prevent tampering.
    
- You can read claims from it to identify and authorize the user.

5. **Summary:**

|Aspect|Server Session|Cookie-based Session|Authentication Cookie|
|---|---|---|---|
|Where session data lives|Server (memory/db)|Client (cookie)|Client (cookie)|
|Cookie content|Just SessionID|All session data (serialized)|Claims/roles (encrypted token)|
|Use case|General session storage|Lightweight session, no server storage|Authentication & authorization|
|Size limit|Depends on server|~4KB max|Usually small (claims only)|
|Security|Server controlled|Must be encrypted and signed|Encrypted and signed|
|Examples in ASP.NET|`HttpContext.Session`|Custom cookie with JSON data|ASP.NET Identity Cookie|

**Extra notes:**

- Storing too much data in a cookie is not recommended due to size and performance.
    
- Sensitive data in cookies **must be encrypted and signed**.
    
- Authentication cookies are a special case of cookies used only for identity claims.
    
- You can combine: use authentication cookie for identity, and a separate session cookie or server session for app data.
## Session Middleware
Here’s how it works:

- In ASP.NET Core, the `HttpContext.Session` property is **always available** once you enable session middleware.
    
- Even if you don’t explicitly use or set any session data, the session **object still exists** in the context.
    
- Initially, the session will be **empty** — it won’t contain any stored key-value pairs.
    
- The session is only **created and stored** (with a session ID and data) when you actually write something into it.
    
- If you never use or write to the session, the server won’t persist any session data, and the client won’t receive a session cookie.

So, `HttpContext.Session` is like an empty container ready to hold session data if you need it — but it doesn’t consume resources until you actually store something.

The **session middleware** in ASP.NET Core is the component that enables session state management for your web application. It intercepts HTTP requests and responses to handle reading and writing session data.

**What is the session middleware?**

- It’s a built-in middleware called `SessionMiddleware`.
    
- You add it to your application’s **middleware pipeline** to enable session support.
    
- It manages:
    
    - Creating the session ID cookie,
        
    - Loading session data from the session store (e.g., in-memory cache),
        
    - Making session data available via `HttpContext.Session`,
        
    - Saving any changes back to the session store,
        
    - Sending the session cookie back to the client.

**How to enable session middleware in ASP.NET Core?**

1. **Add the required services in `Startup.cs` (or `Program.cs` in minimal hosting):**

```csharp
services.AddDistributedMemoryCache(); // Registers a cache to store session data (in-memory)
services.AddSession(options =>
{
    options.IdleTimeout = TimeSpan.FromMinutes(20); // Session expiration time
    options.Cookie.HttpOnly = true;
    options.Cookie.IsEssential = true; // Required for GDPR compliance if you have consent
});
```

2. **Add the middleware to the pipeline:**

```csharp
app.UseSession();
```

Make sure `app.UseSession()` is called **before** `app.UseEndpoints()` or `app.UseMvc()` so the session is available in your controllers.

**Summary**

- The session middleware handles the lifecycle of session state for each request.
    
- You must both **register session services** and **add the session middleware** to enable session support.
    
- Without it, `HttpContext.Session` will not be available or functional.

**You said:**

If the session data is stored entirely in a cookie (with no session information kept on the server), how is that cookie updated?

**ChatGPT said:**

Great question! When you’re using a **cookie-based session**—meaning all the session data is stored entirely inside the cookie on the client, and **no session info is stored on the server**—the way you update session data is different from the server-backed session scenario.

**How do you update a cookie-based session?**

1. **The cookie contains all the session data**, usually serialized (often encrypted and signed to prevent tampering).
    
2. When the client makes a request, it **sends the entire cookie data** to the server.
    
3. The server **reads and decrypts** the cookie to get the current session data.
    
4. When you want to update the session:
    
    - The server **modifies the session data** in memory (the data it extracted from the cookie).
        
    - Then the server **serializes, encrypts, and signs** the updated session data again.
        
    - Finally, the server **sends back a new `Set-Cookie` header** in the HTTP response, containing the updated cookie with the new session data.
        
5. The client browser **overwrites the old cookie** with this updated cookie.

**Key points:**

- Since **all session state lives inside the cookie**, updating the session means **sending a new cookie back to the client** with the updated data.
    
- The client sends the full cookie with each request, so the server always has the latest state.
    
- You must ensure the cookie size stays within browser limits (usually about 4KB).
    
- You typically **encrypt and sign** the cookie to keep the data secure and prevent tampering.

**Contrast with server-backed session:**

|Aspect|Server-backed Session|Cookie-based Session|
|---|---|---|
|Session data location|Stored on server (cache, DB, etc.)|Stored entirely in cookie on client|
|Cookie contents|Only Session ID|All session data|
|Updating session|Update server store|Send updated cookie back to client|
|Server state|Stateful|Stateless|
## Redirect
✅ **What `RedirectToAction("Index")` does:**

When you call:

```csharp
return RedirectToAction("Index");
```

ASP.NET Core returns a **302 Found** (or **303 See Other**) HTTP response to the browser. This response says:

> “Hey browser, I’m not giving you the final result here. Please go make a new GET request to `/Index` instead.”

📦 **Step-by-step flow:**

1. **User makes a request** to `/Privacy`.
    
2. The `Privacy` action executes and calls `RedirectToAction("Index")`.
    
3. ASP.NET Core returns an HTTP 302 response with the **Location header** pointing to `/Index`.
    
4. The browser receives this response and says:
    
    > “Okay, I’ll go to `/Index` now.”
    
5. The browser automatically makes a **new HTTP GET request** to `/Index`.
    
6. The `Index` action runs and returns its view.

🧠 **Important details:**

- This is a **round trip** — it involves **two separate HTTP requests**:
    
    - First: to `/Privacy`
        
    - Second: to `/Index` (triggered by the redirect response)
        
- This is exactly why `TempData` is helpful — it **preserves data between these two separate requests**.
## Roles and Areas
**When to Use Only Roles (Without Areas):**

Use **roles alone** when:

- Your application is **small or medium-sized**.
    
- You have a **limited number of controllers and views**.
    
- You don’t need to separate the UI or logic into distinct sections.
    
- You simply want to control **who can access what** using `[Authorize(Roles = "RoleName")]`.
    

✅ This is simple, effective, and works well for apps with basic role-based permissions.

**When to Use Areas (Along with Roles):**

Use **Areas + Roles** when:

- Your application is **large or growing**.
    
- You need to **logically separate parts of the app**, e.g., `Public`, `Admin`, `Blog`, `Store`, etc.
    
- You want each section to have its **own layout, views, controllers, and routes**.
    
- You aim to keep the project **organized and scalable**.
    
- You still control access using roles, but within each Area.

✅ This provides better structure, maintainability, and flexibility, especially for complex apps.

**Summary Table:**

|Situation|Use Roles Only|Use Areas + Roles|
|---|---|---|
|Small/simple app|✅|❌|
|Few user roles|✅|❌|
|Large/complex app|❌|✅|
|Need UI separation (admin/public)|❌|✅|
|Maintain clean code organization|❌|✅|

In short:

**Roles = Access control**  
**Areas = Logical structure**  

They serve different purposes and often work best when combined in larger apps.
## How Browsers Load Resources After an Initial GET Request
When you make a **GET request** to a website (e.g., typing `https://example.com` into your browser), here's what happens regarding the **HTML and CSS**:

📦 1. **Initial GET request**

The browser sends a GET request to the server:

```
GET / HTTP/1.1
Host: example.com
```

📄 2. **Server response with HTML**

The server responds with an **HTML file** (usually `index.html`) in the **body** of the response:

```http
HTTP/1.1 200 OK
Content-Type: text/html

<!DOCTYPE html>
<html>
  <head>
    <title>Example</title>
    <link rel="stylesheet" href="styles.css">
  </head>
  <body>
    <h1>Hello World</h1>
  </body>
</html>
```

✅ The **HTML** is right there in the response **body**.  
❌ The **CSS file** (`styles.css`) is _not_ included in the HTML response. It’s just referenced.

🧾 3. **Browser parses HTML and triggers more GET requests**

When the browser sees:

```html
<link rel="stylesheet" href="styles.css">
```

It sends another **GET request**:

```
GET /styles.css HTTP/1.1
Host: example.com
```

The server replies with:

```http
HTTP/1.1 200 OK
Content-Type: text/css

body {
  background-color: lightblue;
}
```

The **CSS code is now in the response body** of this second request.

🧠 In short:

- The **HTML file** is loaded in the **response body of the first GET request**.
    
- **CSS files** (and JS, images, etc.) are requested **separately**, each with their **own GET request**, and their content is in the **response bodies** of those respective requests. 
    
- **The browser** parses the HTML and decides what else it needs.

👨‍🏫 **Analogy**

Think of HTML as a recipe:

- It says: “you’ll need 2 cups of CSS and 1 spoon of JavaScript.”
    
- The **browser is the cook** who reads the recipe and says, “Okay, I need to go fetch these ingredients.”

HTTP is just the delivery vehicle — it brings each item when the cook asks for it.
## HTTP/1.1 vs HTTP/2: Request Multiplexing and Connection Management
**HTTP/1.1 and JavaScript requests**

- In **HTTP/1.1**, browsers usually open **multiple parallel TCP connections** (often 6 per domain) to download resources like JS files.
    
- Bundling JS files into **one big file** is a common optimization because it reduces the number of HTTP requests, and since each connection can only download one file at a time, fewer requests means faster loading.
    
- So if you have 10 JS files unbundled, your browser may open multiple HTTP connections and request those files separately, causing more overhead (TCP handshake, TLS negotiation, etc.).

**HTTP/2 and multiple JS files**

- **HTTP/2** allows **multiplexing** over a **single TCP connection**. This means many files can be requested **simultaneously over one connection**, without waiting for previous downloads to finish.
    
- So with HTTP/2, bundling is less critical because the browser and server can handle many parallel requests efficiently over the same connection.
    
- Each JS file is still a separate HTTP request, but these requests share one connection (the same TCP socket).

**What happens in the browser Network tab?**

- In both HTTP/1.1 and HTTP/2, you’ll see **multiple separate resource entries** for each JS file (since they are different files).
    
- But the **“Protocol” column** or “Connection ID” (depending on the browser dev tools) tells you about the connection:
    
    - In **HTTP/1.1**, you’ll see multiple different connections (like `h1`, or different connection IDs).
        
    - In **HTTP/2**, you will usually see the same connection ID or protocol (`h2`), meaning all files are fetched over the same HTTP/2 connection.

**How to tell if multiple JS files were sent over one or several HTTP connections?**

1. **Look at the Protocol or Connection column** in the browser’s Network tab:
    
    - Chrome DevTools for example shows “Protocol” like `h1` (HTTP/1.1) or `h2` (HTTP/2).
        
    - You’ll also see a **Connection ID** or something similar.
        
    - If many JS files have the same connection ID and show `h2`, then they were downloaded over one HTTP/2 connection.
        
    - If files have different connection IDs and `h1`, then multiple HTTP/1.1 connections were used.
        
2. **Inspect the Timing details**:
    
    - In HTTP/1.1, requests often block or wait for a connection slot.
        
    - In HTTP/2, requests can run concurrently on the same connection.

**Summary**

|Aspect|HTTP/1.1|HTTP/2|
|---|---|---|
|Number of connections|Multiple TCP connections per domain|Single TCP connection with multiplexing|
|JS files requests|Multiple, each often over different connections|Multiple, multiplexed over one connection|
|Bundling necessity|Bundling reduces requests and overhead|Bundling less critical due to multiplexing|
|Browser Network tab|Multiple connections, Protocol = `h1`|One connection, Protocol = `h2`|

**You said:**

So, we will still see the same number of HTTP requests and responses, but the key point is whether they were made over one HTTP connection or several?

**ChatGPT said:**

Exactly! You nailed it:

- **Number of HTTP requests and responses stays the same** because each JS file is still a separate resource requested individually.
    
- The big difference with HTTP/2 is that **all those requests can be multiplexed over a single HTTP connection**, instead of opening many separate connections like in HTTP/1.1.
    
- So yes, the key part you look for is **whether those multiple requests happen over one single HTTP/2 connection or spread across multiple HTTP/1.1 connections**.

This multiplexing reduces overhead and latency, making loading multiple files faster **without needing to bundle them into one big file**.

If you open your browser Network tab:

- You’ll see the same number of JS files requested (same number of rows).
    
- But the **protocol column or connection ID** tells you if it was one HTTP/2 connection (`h2`) or multiple HTTP/1.1 connections (`h1`).

**So the connection lifecycle looks like this:**

1. Establish connection (TCP handshake)
    
2. Send one or more HTTP requests (depending on HTTP version)
    
3. Wait for corresponding responses
    
4. Keep connection open for reuse or close it (either gracefully or by timeout)

The response **must always come back on the same connection** where the request was sent.

Here’s why:

- HTTP is a request-response protocol bound to a TCP connection.
    
- When the client sends a request over a TCP connection, the server replies on **that same TCP connection**.
    
- You cannot send a request on one connection and receive the response on another — the protocol requires matching pairs on the same connection.

Even with HTTP/2 multiplexing (many requests on one connection), **all responses come back through that same single connection**, just interleaved.

If a connection closes or drops, the client usually needs to open a new connection and resend the request.

So in short:

**Request and response always travel on the same connection.**
# Bookmarks
[AspNetCore.Docs/aspnetcore/security/gdpr/sample at live · dotnet/AspNetCore.Docs](https://github.com/dotnet/AspNetCore.Docs/tree/live/aspnetcore/security/gdpr/sample) - GDPR sample app.

Completion: 12.06.2025